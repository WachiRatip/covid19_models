{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import requests\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "URL = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/\"\n",
    "CONFIRMED=\"time_series_covid19_confirmed_global.csv\" \n",
    "DEATH=\"time_series_covid19_deaths_global.csv\"\n",
    "RECOVERED=\"time_series_covid19_recovered_global.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from GitHub which was provided by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).\n",
    "if not os.path.isfile(f\"./dataset/{CONFIRMED}\"):\n",
    "    for name in [CONFIRMED, DEATH, RECOVERED]:\n",
    "        url = os.path.join(URL, name)\n",
    "        r = requests.get(url)\n",
    "        file_csv = open(os.path.join(\"./dataset\", name), 'wb')\n",
    "        file_csv.write(r.content)\n",
    "        file_csv.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train and test in Mathematical, Statistical and Machine learning model\n",
    "temp_time_series_csv = pd.read_csv(os.path.join(\"dataset\",CONFIRMED)).iloc[:, 4:].sum(axis=0)\n",
    "TOTAL_DAYS = int(temp_time_series_csv.shape[0])\n",
    "print(u\"The number of days in the time-series data is {}.\".format(TOTAL_DAYS))\n",
    "## set test size is 14 days\n",
    "TEST_SIZE = 14\n",
    "TRAIN_SIZE = int(TOTAL_DAYS - TEST_SIZE)\n",
    "print(u\"We separate time-series data into 2 sets; Train and Test which their size are {} and {} respectively.\".format(TRAIN_SIZE, TEST_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constant values\n",
    "def set_seir(reproductive_number = 2):\n",
    "    ## Total of population the world; scr: https://www.worldometers.info/world-population/\n",
    "    population = 7768726098\n",
    "    rate_of_brith = 1.05\n",
    "    print(u\"population of the world is {}.\".format(population))\n",
    "\n",
    "    ## recovery rate is the mean of recovered and death cases of the total cases\n",
    "    _death = pd.read_csv(os.path.join(\"dataset\",DEATH)).iloc[:, 4:].sum(axis=0)\n",
    "    _death = _death.diff().fillna(_death[0]).astype(np.int64)\n",
    "    _recovery = pd.read_csv(os.path.join(\"dataset\",RECOVERED)).iloc[:, 4:].sum(axis=0)\n",
    "    _recovery = _recovery.diff().fillna(_recovery[0]).astype(np.int64)\n",
    "    _recovery_and_death = _death + _recovery\n",
    "    _recovery_and_death = _recovery_and_death.pct_change().fillna(\"0\").astype(np.int64)\n",
    "    rate_of_recover_and_death = _recovery_and_death.mean()\n",
    "    print(u\"recovery rate is {}.\".format(rate_of_recover_and_death))\n",
    "\n",
    "    ## incubation rate is the mean of confirmed cases of the total cases\n",
    "    _confirmed = pd.read_csv(os.path.join(\"dataset\",CONFIRMED)).iloc[:, 4:].sum(axis=0)\n",
    "    _confirmed = _confirmed.diff().fillna(_confirmed[0]).astype(np.int64)\n",
    "    _confirmed = _confirmed.pct_change().fillna(\"0\").astype(np.int64)\n",
    "    rate_of_incubation = _confirmed.mean()\n",
    "    print(u\"incubation rate is {}.\".format(rate_of_incubation))\n",
    "\n",
    "    ## contact rate will calculated by assume that R_0 = 2 then contact rate = R_0 * reconvery rate\n",
    "    rate_of_contactive =  reproductive_number * rate_of_recover_and_death\n",
    "    print(u\"contact rate is {}.\".format(rate_of_contactive))\n",
    "    return population, rate_of_contactive, rate_of_incubation, rate_of_recover_and_death\n",
    "\n",
    "def seir_model(day_zero = -1):\n",
    "    ''' SEIR model without vital dynamics '''\n",
    "    # I've adapted code from original sorces https://scipython.com/book/chapter-8-scipy/additional-examples/the-sir-epidemic-model/\n",
    "    # and https://institutefordiseasemodeling.github.io/Documentation/general/model-seir.html#id11\n",
    "\n",
    "    # Total population, N; \n",
    "    N = population\n",
    "    # Initial number of infected, exposed and recovered individuals, I0, E0 and R0.\n",
    "    ## Initial rnumber of recovered cases is the number of recovered plus death cases in the lastest day in the time series \n",
    "    ### day_zero = -1 means use the lastest day in the time-series as day 0 of epidemic\n",
    "    R0 = pd.read_csv(os.path.join(\"dataset\",DEATH)).iloc[:, day_zero].sum(axis=0).astype(np.int64) +\\\n",
    "            pd.read_csv(os.path.join(\"dataset\",RECOVERED)).iloc[:, day_zero].sum(axis=0).astype(np.int64)\n",
    "    ## Initial rnumber of infected cases is confirmed minus death and recovered cases in lastest day in the time series \n",
    "    I0 = pd.read_csv(os.path.join(\"dataset\",CONFIRMED)).iloc[:, day_zero].sum(axis=0).astype(np.int64) - R0\n",
    "    ## assume that confirmed cases is nothing but merely the tip of the iceberg; therefore, let confirmed infected group is ten percent of exposed group.\n",
    "    E0 = 10 * I0 \n",
    "    S0 = N - I0 - R0 - E0\n",
    "    print(u\"Initial number of infected, exposed and recovered individuals are {}, {}, and {} respectively.\".format(I0, E0, R0))\n",
    "    print(u\"Now, the initial number of susceptible individuals is {}.\".format(S0))\n",
    "\n",
    "    # Average contact rate(beta), incubation rate(alpha) and recovery rate(gamma) (in 1/days); \n",
    "    beta = rate_of_contactive\n",
    "    alpha = rate_of_incubation\n",
    "    gamma = rate_of_recover_and_death\n",
    "\n",
    "    # A grid of time points (in days)\n",
    "    DAYS_TO_PREDICT = 42 + 28 ## use the answer to life the universe and everything + 28 days later\n",
    "    t = np.linspace(0, DAYS_TO_PREDICT, DAYS_TO_PREDICT)\n",
    "\n",
    "    # The SIR model differential equations.\n",
    "    def deriv(y, t, N, alpha, beta, gamma):\n",
    "        S, E, I, R = y\n",
    "        dSdt = -beta * S * I / N\n",
    "        dEdt = beta * S *  I / N - alpha * E\n",
    "        dIdt = alpha * E - gamma * I\n",
    "        dRdt = gamma * I\n",
    "        return dSdt, dEdt, dIdt, dRdt\n",
    "\n",
    "    # Initial conditions vector\n",
    "    y0 = S0, E0, I0, R0\n",
    "    # Integrate the SIR equations over the time grid, t.\n",
    "    ret = odeint(deriv, y0, t, args=(N, alpha, beta, gamma))\n",
    "    S, E, I, R = ret.T\n",
    "\n",
    "    # Plot the data on three separate curves for S(t), I(t) and R(t)\n",
    "    plt.plot(t, S/1000000, 'b', alpha=0.5, lw=2, label='Susceptible')\n",
    "    plt.plot(t, E/1000000, 'y', alpha=0.5, lw=2, label='Exposed')\n",
    "    plt.plot(t, I/1000000, 'r', alpha=0.5, lw=2, label='Infected')\n",
    "    plt.plot(t, R/1000000, 'g', alpha=0.5, lw=2, label='Recovered with immunity or Death')\n",
    "    plt.xlabel('days from now')\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.grid(b=True, which='major', c='w', lw=2, ls='-')\n",
    "    plt.legend()\n",
    "    plt.title(\"SEIR model\")\n",
    "    plt.savefig(\"seir_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    ## from now on model\n",
    "    if day_zero != -TEST_SIZE:\n",
    "        plt.plot(t, I/1000000, 'r', alpha=0.5, lw=2, label='Predicted Infected cases')\n",
    "        plt.xlabel('days from now')\n",
    "        plt.ylabel('Number (millionth)')\n",
    "        plt.legend()\n",
    "        plt.title(\"SEIR model for prediction infected cases (future)\")\n",
    "        plt.savefig(\"future_seir_model.jpg\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    else:\n",
    "        total_confirmed = cases(csv=CONFIRMED, name=\"confirmed\")[-TEST_SIZE:]\n",
    "        total_death = cases(csv=DEATH, name=\"deaths\")[-TEST_SIZE:]\n",
    "        total_recovered = cases(csv=RECOVERED, name=\"recovered\")[-TEST_SIZE:]\n",
    "        real_I = total_confirmed - total_death - total_recovered\n",
    "        plt.plot(real_I.index[:TEST_SIZE], real_I/1000000, 'r', alpha=0.5, lw=2, label='Real Infected cases')\n",
    "        plt.plot(real_I.index[:TEST_SIZE], I[:TEST_SIZE]/1000000, 'b', alpha=0.5, lw=2, label='Predicted Infected cases')\n",
    "        plt.xlabel(u'{} days ago to now'.format(TEST_SIZE))\n",
    "        plt.ylabel('Number (millionth)')\n",
    "        plt.title(\"SEIR model for prediction infected cases (validation)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"validation_seir_model.jpg\")\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cases(csv, name):\n",
    "    # load dataset\n",
    "    df = pd.read_csv(os.path.join(\"dataset\",csv))\n",
    "    #print(df.head())\n",
    "\n",
    "    # select only time series parts\n",
    "    df = df.iloc[:, 4:]\n",
    "\n",
    "    # check missing values\n",
    "    if df.isnull().sum().sum():\n",
    "        print(\"There are missing values, do some thing with them.\")\n",
    "\n",
    "    # get number of cases of the day\n",
    "    cases = df.sum(axis=0)\n",
    "    \n",
    "    # show graph of confirmed cases\n",
    "    plt.plot(cases)\n",
    "    plt.title(name+\" cases\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(name+\"_cases.jpg\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # get only number of new cases of the day\n",
    "    daily_cases = cases.diff().fillna(cases[0]).astype(np.int64)\n",
    "\n",
    "    # show graph of daily confirmed cases\n",
    "    plt.plot(daily_cases)\n",
    "    plt.title(\"daily \"+name+\" cases\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(\"daily_\"+name+\"_cases.jpg\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # show graph of daily and total of the day\n",
    "    plt.plot(cases, label=\"Total cases\")\n",
    "    plt.plot(daily_cases, label=\"Daily cases\")\n",
    "    plt.title(\"Daily and total \"+name+\" cases\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"daily_total_\"+name+\"_cases.jpg\")\n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return cases\n",
    "\n",
    "def build_lstm(time_series_data, name):\n",
    "    print(\"+\"*50)\n",
    "    print(u\"Building LSTM model for {} group.\".format(name))\n",
    "    ''' Long-short Term Memory network '''\n",
    "    ## I've adapted code from original source: https://colab.research.google.com/drive/1nQYJq1f7f4R0yeZOzQ9rBKgk00AfLoS0#scrollTo=iPJMdlBEErg3\n",
    "    train = time_series_data[:-TEST_SIZE]\n",
    "    test = time_series_data[-TEST_SIZE:]\n",
    "\n",
    "    # preprocessing data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(np.expand_dims(train, axis=1))\n",
    "    train = scaler.transform(np.expand_dims(train, axis=1))\n",
    "    test = scaler.transform(np.expand_dims(test, axis=1))\n",
    "\n",
    "    def create_sequences(data, seq_length):\n",
    "        xs = []\n",
    "        ys = []\n",
    "\n",
    "        for i in range(len(data)-seq_length-1):\n",
    "            x = data[i:(i+seq_length)]\n",
    "            y = data[i+seq_length]\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "        return np.array(xs), np.array(ys)\n",
    "\n",
    "    # data preparing\n",
    "    seq_length = 5\n",
    "    X_train, y_train = create_sequences(train, seq_length)\n",
    "    X_test, y_test = create_sequences(test, seq_length)\n",
    "\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "    # model training\n",
    "    ## define recurrent neural network architecture\n",
    "    class CoronaVirusPredictor(nn.Module):\n",
    "\n",
    "        def __init__(self, n_features, n_hidden, seq_len, n_layers=2):\n",
    "            super(CoronaVirusPredictor, self).__init__()\n",
    "\n",
    "            self.n_hidden = n_hidden\n",
    "            self.seq_len = seq_len\n",
    "            self.n_layers = n_layers\n",
    "\n",
    "            self.lstm = nn.LSTM(\n",
    "                        input_size=n_features,\n",
    "                        hidden_size=n_hidden,\n",
    "                        num_layers=n_layers,\n",
    "                        dropout=0.5\n",
    "                        )\n",
    "\n",
    "            self.linear = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "\n",
    "        def reset_hidden_state(self):\n",
    "            self.hidden = (\n",
    "                            torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n",
    "                            torch.zeros(self.n_layers, self.seq_len, self.n_hidden)\n",
    "                        )\n",
    "\n",
    "        def forward(self, sequences):\n",
    "            lstm_out, self.hidden = self.lstm(\n",
    "                                    sequences.view(len(sequences), self.seq_len, -1),\n",
    "                                    self.hidden\n",
    "                                    )\n",
    "\n",
    "            last_time_step = \\\n",
    "                lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n",
    "\n",
    "            y_pred = self.linear(last_time_step)\n",
    "            return y_pred\n",
    "\n",
    "    \n",
    "    ## define training method\n",
    "    def train_model(model, train, train_labels, test_data=None, test_labels=None):\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        num_epochs = 120\n",
    "\n",
    "        train_hist = np.zeros(num_epochs)\n",
    "        test_hist = np.zeros(num_epochs)\n",
    "\n",
    "        for t in range(num_epochs):\n",
    "            model.reset_hidden_state()\n",
    "\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            loss = loss_fn(y_pred.float(), y_train)\n",
    "\n",
    "            if test_data is not None:\n",
    "                with torch.no_grad():\n",
    "                    y_test_pred = model(X_test)\n",
    "                    test_loss = loss_fn(y_test_pred.float(), y_test)\n",
    "\n",
    "                test_hist[t] = test_loss.item()\n",
    "\n",
    "                if t % 10 == 0:  \n",
    "                    print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n",
    "\n",
    "            elif t % 10 == 0:\n",
    "                print(f'Epoch {t} train loss: {loss.item()}')\n",
    "\n",
    "            train_hist[t] = loss.item()\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimiser.step()\n",
    "        \n",
    "        return model.eval(), train_hist, test_hist\n",
    "\n",
    "    ## building step here\n",
    "    model = CoronaVirusPredictor(n_features=1, n_hidden=512, seq_len=seq_length, n_layers=2)\n",
    "    model, train_hist, test_hist = train_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "    # prediction step\n",
    "    def predict(data_seq, predict_len):\n",
    "        seq = data_seq\n",
    "        preds = []\n",
    "        for _ in range(predict_len):\n",
    "            y_pred = model(seq)\n",
    "            pred = torch.flatten(y_pred).item()\n",
    "            preds.append(pred)\n",
    "            new_seq = seq.numpy().flatten()\n",
    "            new_seq = np.append(new_seq, [pred])\n",
    "            new_seq = new_seq[1:]\n",
    "            seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()\n",
    "\n",
    "        predicted_cases = scaler.inverse_transform(np.expand_dims(preds, axis=0)).flatten()\n",
    "        return predicted_cases\n",
    "\n",
    "\n",
    "    #self_predicted_cases = predict(X_train[:1], TRAIN_SIZE)\n",
    "    validation_predicted_cases = predict(X_test[:1], TEST_SIZE)\n",
    "    #validation_predicted_cases = np.append(self_predicted_cases, validation_predicted_cases)\n",
    "    plt.plot(time_series_data[-TEST_SIZE:]/1000000, label=\"Real cases\")\n",
    "    plt.plot(validation_predicted_cases/1000000, label=\"Predicted cases\")\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.xlabel(\"14 days ago to now\")\n",
    "    plt.title(\"LSTM model for \"+name+\" cases\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name+\"_lstm_model_validation.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(np.expand_dims(time_series_data, axis=1))\n",
    "    all_data = scaler.transform(np.expand_dims(time_series_data, axis=1))\n",
    "    \n",
    "    X_all, y_all = create_sequences(all_data, seq_length)\n",
    "    X_all = torch.from_numpy(X_all).float()\n",
    "    y_all = torch.from_numpy(y_all).float()\n",
    "    \n",
    "    model = CoronaVirusPredictor(n_features=1, n_hidden=512, seq_len=seq_length, n_layers=2)\n",
    "    model, train_hist, _ = train_model(model, X_all, y_all)\n",
    "\n",
    "    DAYS_TO_PREDICT = 42 + 28\n",
    "    t = np.linspace(0, DAYS_TO_PREDICT, DAYS_TO_PREDICT)\n",
    "    predicted_cases = predict(X_test[-1:], DAYS_TO_PREDICT)\n",
    "    plt.plot(t, predicted_cases/1000000, label='Predicted Cases')\n",
    "    plt.xlabel(\"days from now\")\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.title(\"LSTM model for \"+name+\" cases\")\n",
    "    plt.legend()\n",
    "    plt.savefig(name+\"_lstm_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return validation_predicted_cases, predicted_cases\n",
    "\n",
    "def lstm(time_series_data=[], validation_predicted_cases=[], predicted_cases=[]):\n",
    "    time_series = time_series_data[0] - time_series_data[1] - time_series_data[2]\n",
    "    validation_cases = validation_predicted_cases[0] - validation_predicted_cases[1] - validation_predicted_cases[2]\n",
    "    future_cases = predicted_cases[0] - predicted_cases[1] - predicted_cases[2]\n",
    "    # plot validation graph\n",
    "    plt.plot(time_series[-TEST_SIZE:]/1000000, label=\"Real cases\")\n",
    "    plt.plot(validation_cases/1000000, label=\"Predicted cases\")\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.xlabel(\"14 days ago to now\")\n",
    "    plt.title(\"LSTM model for prediction infected cases (validation)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"validation_lstm_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # plot prediction graph\n",
    "    DAYS_TO_PREDICT = 42 + 28\n",
    "    t = np.linspace(0, DAYS_TO_PREDICT, DAYS_TO_PREDICT)\n",
    "    plt.plot(t, future_cases/1000000, label='Predicted Cases')\n",
    "    plt.xlabel(\"days from now\")\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.title(\"LSTM model for prediction infected cases (future)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"future_lstm_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stationary(total_cases):\n",
    "    ''' ARIMA model '''\n",
    "    # I've adapted code from original source: https://towardsdatascience.com/arima-forecasting-in-python-90d36c2246d3,\n",
    "    # https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n",
    "    # https://towardsdatascience.com/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7\n",
    "    # https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/\n",
    "    # and https://www.statsmodels.org/stable/tsa.html\n",
    "\n",
    "    SIGNIFICANCE = 0.05\n",
    "\n",
    "    rolling_mean = total_cases.rolling(window = 7).mean()\n",
    "    rolling_std = total_cases.rolling(window = 7).std()\n",
    "    plt.plot(total_cases, color = 'blue', label = 'Original')\n",
    "    plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\n",
    "    plt.plot(rolling_std, color = 'black', label = 'Rolling Std')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('Rolling Mean & Rolling Standard Deviation')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    result = adfuller(total_cases)\n",
    "    print('ADF Statistic: {}'.format(result[0]))\n",
    "    print('p-value: {}'.format(result[1]))\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t{}: {}'.format(key, value))\n",
    "\n",
    "    # let's calculate p-value to check if the timeseries is stationary\n",
    "    if result[1] >= SIGNIFICANCE:\n",
    "        print(\"This time-series is not stationary, d>0\")\n",
    "    else:\n",
    "        print(\"This time-series is stationary, d=0\")\n",
    "        \n",
    "def find_order(total_cases, partial=True):\n",
    "    if not partial:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "        ax1 = fig.add_subplot(311)\n",
    "        fig = plot_acf(total_cases, ax=ax1,\n",
    "                       title=\"Prtial Autocorrelation on Original Series\") \n",
    "        ax2 = fig.add_subplot(312)\n",
    "        fig = plot_acf(total_cases.diff().dropna(), ax=ax2, \n",
    "                       title=\"1st Order Differencing\")\n",
    "\n",
    "        ax3 = fig.add_subplot(313)\n",
    "        fig = plot_acf(total_cases.diff().diff().dropna(), ax=ax3, \n",
    "                       title=\"2nd Order Differencing\")\n",
    "\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "        ax1 = fig.add_subplot(311)\n",
    "        fig = plot_pacf(total_cases, ax=ax1,\n",
    "                       title=\"Prtial Autocorrelation on Original Series\") \n",
    "        ax2 = fig.add_subplot(312)\n",
    "        fig = plot_pacf(total_cases.diff().dropna(), ax=ax2, \n",
    "                       title=\"1st Order Differencing\")\n",
    "\n",
    "        ax3 = fig.add_subplot(313)\n",
    "        fig = plot_pacf(total_cases.diff().diff().dropna(), ax=ax3, \n",
    "                       title=\"2nd Order Differencing\")\n",
    "        \n",
    "def acf(total_cases):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    ax1 = fig.add_subplot(321)\n",
    "    fig = plot_acf(total_cases, ax=ax1,\n",
    "                   title=\"Autocorrelation on Original Series\") \n",
    "    ax2 = fig.add_subplot(322)\n",
    "    ax2.plot(total_cases)\n",
    "    ax2.set_title(\"Autocorrelation on Original Series\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    ax3 = fig.add_subplot(323)\n",
    "    fig = plot_acf(total_cases.diff().dropna(), ax=ax3, \n",
    "                   title=\"1st Order Differencing\")\n",
    "    ax4 = fig.add_subplot(324)\n",
    "    ax4.plot(total_cases.diff().dropna())\n",
    "    ax4.set_title(\"1st Order Differencing\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    ax5 = fig.add_subplot(325)\n",
    "    fig = plot_acf(total_cases.diff().diff().dropna(), ax=ax5, \n",
    "                   title=\"2nd Order Differencing\")\n",
    "    ax6 = fig.add_subplot(326)\n",
    "    ax6.plot(total_cases.diff().diff().dropna())\n",
    "    ax6.set_title(\"2nd Order Differencing\")\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "def pacf(total_cases):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    ax1 = fig.add_subplot(321)\n",
    "    fig = plot_pacf(total_cases, ax=ax1,\n",
    "                   title=\"Partial Autocorrelation on Original Series\") \n",
    "    ax2 = fig.add_subplot(322)\n",
    "    ax2.plot(total_cases)\n",
    "    ax2.set_title(\"Partial Autocorrelation on Original Series\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    ax3 = fig.add_subplot(323)\n",
    "    fig = plot_pacf(total_cases.diff().dropna(), ax=ax3, \n",
    "                   title=\"1st Order Differencing\")\n",
    "    ax4 = fig.add_subplot(324)\n",
    "    ax4.plot(total_cases.diff().dropna())\n",
    "    ax4.set_title(\"1st Order Differencing\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    ax5 = fig.add_subplot(325)\n",
    "    fig = plot_pacf(total_cases.diff().diff().dropna(), ax=ax5, \n",
    "                   title=\"2nd Order Differencing\")\n",
    "    ax6 = fig.add_subplot(326)\n",
    "    ax6.plot(total_cases.diff().diff().dropna())\n",
    "    ax6.set_title(\"2nd Order Differencing\")\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "\n",
    "def pdq_analysis(total_cases):\n",
    "    # Find the order of differencing, d:\n",
    "    print(\"WARNING!:  if the lag 1 autocorrelation itself is too negative, then the series is probably over-differenced.\")\n",
    "    print(\"Finding the order of differencing (d) in ARIMA model\")\n",
    "    acf(total_cases)\n",
    "    \n",
    "    # Find the order of the MA, q:\n",
    "    print(\"Finding the order of the MA term (q)\")\n",
    "    #acf(total_cases)\n",
    "        \n",
    "    # Find the order of AR; p\n",
    "    print(\"Finding the order of the AR term (p)\")\n",
    "    pacf(total_cases)\n",
    "    \n",
    "    \n",
    "def build_arima(total_cases, name, order): \n",
    "    # future\n",
    "    DAYS_TO_PREDICT = 42+28\n",
    "    model = ARIMA(total_cases, order=order)\n",
    "    results = model.fit(disp=0)\n",
    "    print(results.summary())\n",
    "    \n",
    "    residuals = pd.DataFrame(results.resid)\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "    residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    figure = results.plot_predict(1, TOTAL_DAYS+DAYS_TO_PREDICT, dynamic=False)\n",
    "    plt.title(\"ARIMA model for predition infected cases (future)\")\n",
    "    plt.savefig(name+\"_arima_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    fc, _, _ = results.forecast(DAYS_TO_PREDICT, alpha=0.05)\n",
    "    return fc\n",
    "    \n",
    "def build_arima_valiadation(total_cases, name, order):\n",
    "    # validation\n",
    "    model = ARIMA(total_cases[:-TEST_SIZE], order=order)\n",
    "    results = model.fit(disp=0)\n",
    "    print(results.summary())\n",
    "    fc, se, conf = results.forecast(TEST_SIZE, alpha=0.05)  # 95% conf\n",
    "    fc_series = pd.Series(fc, index=total_cases[-TEST_SIZE:].index)\n",
    "    lower_series = pd.Series(conf[:, 0], index=total_cases[-TEST_SIZE:].index)\n",
    "    upper_series = pd.Series(conf[:, 1], index=total_cases[-TEST_SIZE:].index)\n",
    "    plt.figure(figsize=(12,5), dpi=100)\n",
    "    plt.plot(total_cases[:-TEST_SIZE], label='training')\n",
    "    plt.plot(total_cases[-TEST_SIZE:], label='actual')\n",
    "    plt.plot(fc_series, label='forecast')\n",
    "    plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                     color='k', alpha=.15)\n",
    "    plt.title('ARIMA model for predition infected cases (validation)')\n",
    "    plt.legend(loc='upper left', fontsize=8)\n",
    "    plt.savefig(name+\"_varidation_arima_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return fc\n",
    "    \n",
    "def arima(time_series_data=[], validation_predicted_cases=[], predicted_cases=[]):\n",
    "    time_series = time_series_data[0] - time_series_data[1] - time_series_data[2]\n",
    "    validation_cases = validation_predicted_cases[0] - validation_predicted_cases[1] - validation_predicted_cases[2]\n",
    "    future_cases = predicted_cases[0] - predicted_cases[1] - predicted_cases[2]\n",
    "    # plot validation graph\n",
    "    plt.plot(time_series[-TEST_SIZE:]/1000000, label=\"Real cases\")\n",
    "    plt.plot(validation_cases/1000000, label=\"Predicted cases\")\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.xlabel(\"14 days ago to now\")\n",
    "    plt.title(\"ARIMA model for prediction infected cases (validation)\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"validation_arima_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # plot prediction graph\n",
    "    DAYS_TO_PREDICT = 42 + 28\n",
    "    t = np.linspace(0, DAYS_TO_PREDICT, DAYS_TO_PREDICT)\n",
    "    plt.plot(t, future_cases/1000000, label='Predicted Cases')\n",
    "    plt.xlabel(\"days from now\")\n",
    "    plt.ylabel('Number (millionth)')\n",
    "    plt.title(\"ARIMA model for prediction infected cases (future)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"future_arima_model.jpg\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\"*100)\n",
    "# mathematical model; SEIR model, a compartmental models in epidemiology.\n",
    "population, rate_of_contactive, rate_of_incubation, rate_of_recover_and_death = set_seir(reproductive_number = 2)\n",
    "seir_model(day_zero = -TEST_SIZE ) ## model from 14 days ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seir_model() ## model from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\"*100)\n",
    "# machine learning model; Long-short Term Memory network (as known as LSTM model), a type of recurrent neural network designed model.\n",
    "total_confirmed = cases(csv=CONFIRMED, name=\"confirmed\")\n",
    "total_death = cases(csv=DEATH, name=\"deaths\")\n",
    "total_recovered = cases(csv=RECOVERED, name=\"recovered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_validation_predicted_cases, confirmed_predicted_cases = build_lstm(total_confirmed, name=\"confirmed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_validation_predicted_cases, deaths_predicted_cases = build_lstm(total_death, name=\"deaths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_validation_predicted_cases, recovered_predicted_cases = build_lstm(total_recovered, name=\"recovered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm(time_series_data=[total_confirmed, total_death, total_recovered],\n",
    "     validation_predicted_cases=[confirmed_validation_predicted_cases, deaths_validation_predicted_cases, recovered_validation_predicted_cases],\n",
    "     predicted_cases=[confirmed_predicted_cases, deaths_predicted_cases, recovered_predicted_cases])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#\"*100)\n",
    "# statistical model; ARIMA model, a time-series model.\n",
    "total_confirmed = cases(csv=CONFIRMED, name=\"confirmed\")\n",
    "total_death = cases(csv=DEATH, name=\"deaths\")\n",
    "total_recovered = cases(csv=RECOVERED, name=\"recovered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_stationary(total_confirmed)\n",
    "pdq_analysis(total_confirmed)\n",
    "order = (1,1,1)\n",
    "arima_confirmed = build_arima(total_cases=total_confirmed, name=\"confirmed\", order=order)\n",
    "arima_confirmed_validation = build_arima_valiadation(total_cases=total_confirmed, name=\"confirmed\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_stationary(total_death)\n",
    "pdq_analysis(total_death)\n",
    "order = (1,1,1)\n",
    "arima_death = build_arima(total_cases=total_death, name=\"deaths\", order=order)\n",
    "arima_death_validation = build_arima_valiadation(total_cases=total_death, name=\"deaths\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_stationary(total_recovered)\n",
    "pdq_analysis(total_recovered)\n",
    "order = (1,1,1)\n",
    "arima_recovered = build_arima(total_cases=total_recovered, name=\"recovered\", order=order)\n",
    "arima_recovered_validation = build_arima_valiadation(total_cases=total_recovered, name=\"recovered\", order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima(time_series_data=[total_confirmed, total_death, total_recovered], \n",
    "      validation_predicted_cases=[arima_confirmed_validation, arima_death_validation, arima_recovered_validation], \n",
    "      predicted_cases=[arima_confirmed, arima_death, arima_recovered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "interpreter": {
   "hash": "5d97ad4eda96f4e0dcd5ae4f97368654619500468c6147550d2a1b2a1881f9a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}